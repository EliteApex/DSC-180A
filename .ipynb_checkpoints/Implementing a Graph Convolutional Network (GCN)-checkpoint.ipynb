{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d48fe682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9440255165100098\n",
      "Epoch 10, Loss: 0.5989371538162231\n",
      "Epoch 20, Loss: 0.09909845143556595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 0.021152963861823082\n",
      "Epoch 40, Loss: 0.007933911867439747\n",
      "Epoch 50, Loss: 0.004476470407098532\n",
      "Epoch 60, Loss: 0.0031994658056646585\n",
      "Epoch 70, Loss: 0.00258433073759079\n",
      "Epoch 80, Loss: 0.0022107036784291267\n",
      "Epoch 90, Loss: 0.0019445634679868817\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(dataset.data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        flag_is_correct = pred[dataset.data.test_mask] == dataset.data.y[dataset.data.test_mask]\n",
    "        acc = int(flag_is_correct.sum()) / int(dataset.data.test_mask.sum())\n",
    "        return acc, pred[dataset.data.test_mask], dataset.data.y[dataset.data.test_mask]\n",
    "\n",
    "\n",
    "acc_lst = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    acc_lst.append(test(model)[0])\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd458061-8d0e-4ff5-b9c1-df37ceb239ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.519,\n",
       " 0.622,\n",
       " 0.605,\n",
       " 0.583,\n",
       " 0.613,\n",
       " 0.661,\n",
       " 0.705,\n",
       " 0.733,\n",
       " 0.755,\n",
       " 0.765,\n",
       " 0.771,\n",
       " 0.779,\n",
       " 0.778,\n",
       " 0.777,\n",
       " 0.775,\n",
       " 0.774,\n",
       " 0.773,\n",
       " 0.773,\n",
       " 0.772,\n",
       " 0.772,\n",
       " 0.772,\n",
       " 0.772,\n",
       " 0.77,\n",
       " 0.77,\n",
       " 0.768,\n",
       " 0.768,\n",
       " 0.766,\n",
       " 0.766,\n",
       " 0.767,\n",
       " 0.769,\n",
       " 0.769,\n",
       " 0.769,\n",
       " 0.771,\n",
       " 0.769,\n",
       " 0.766,\n",
       " 0.766,\n",
       " 0.766,\n",
       " 0.766,\n",
       " 0.766,\n",
       " 0.765,\n",
       " 0.765,\n",
       " 0.765,\n",
       " 0.765,\n",
       " 0.765,\n",
       " 0.765,\n",
       " 0.765,\n",
       " 0.764,\n",
       " 0.764,\n",
       " 0.765,\n",
       " 0.765,\n",
       " 0.765,\n",
       " 0.765,\n",
       " 0.766,\n",
       " 0.766,\n",
       " 0.767,\n",
       " 0.767,\n",
       " 0.766,\n",
       " 0.766,\n",
       " 0.765,\n",
       " 0.765,\n",
       " 0.764,\n",
       " 0.764,\n",
       " 0.764,\n",
       " 0.764,\n",
       " 0.764,\n",
       " 0.764,\n",
       " 0.764,\n",
       " 0.764,\n",
       " 0.766,\n",
       " 0.767,\n",
       " 0.767,\n",
       " 0.767,\n",
       " 0.768,\n",
       " 0.768,\n",
       " 0.768,\n",
       " 0.768,\n",
       " 0.768,\n",
       " 0.769,\n",
       " 0.769,\n",
       " 0.769,\n",
       " 0.769,\n",
       " 0.769,\n",
       " 0.769,\n",
       " 0.769,\n",
       " 0.769,\n",
       " 0.769,\n",
       " 0.77,\n",
       " 0.77,\n",
       " 0.769,\n",
       " 0.768,\n",
       " 0.768,\n",
       " 0.768,\n",
       " 0.768,\n",
       " 0.768,\n",
       " 0.768,\n",
       " 0.768,\n",
       " 0.768,\n",
       " 0.769,\n",
       " 0.768,\n",
       " 0.768]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc18442",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "GCN aggregates features from a node’s neighbors using graph convolutions. This allows the network to learn representations based on both node features and graph structure.\n",
    "The Cora dataset is used to classify nodes into one of 7 research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb882b",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "\n",
    "1. What would happen if we added more GCN layers (e.g., 3 layers instead of 2)? How would this affect over-smoothing?\n",
    "2. What would happen if we used a larger hidden dimension (e.g., 64 instead of 16)? How would this impact the model's capacity?\n",
    "3. What would happen if we replaced ReLU activation with a sigmoid function? Would the performance change?\n",
    "\n",
    "4. What would happen if we trained on only 10% of the nodes and tested on the remaining 90%? How would the performance be affected?\n",
    "5. What would happen if we used a different optimizer (e.g., RMSprop) instead of Adam? Would it affect the convergence speed?\n",
    "\n",
    "Extra credit: \n",
    "1. What would happen if we used edge weights (non-binary) in the adjacency matrix? How would it affect message passing?\n",
    "2. What would happen if we removed the log-softmax function in the output layer? Would the loss function still work correctly?\n",
    "\n",
    "## No points, just for you to think about:\n",
    "1. What would happen if we applied dropout to the node features during training? How would it affect the model’s generalization?\n",
    "2. What would happen if we used mean-pooling instead of summing the messages in the GCN layers?\n",
    "3. What would happen if we pre-trained the node features using a different algorithm, like Node2Vec, before feeding them into the GCN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832dcb0-90df-47ac-8c1e-8f8aaba925fb",
   "metadata": {},
   "source": [
    "## Answers ## \n",
    "1) Adding more layers would mean each node gets more messages passed to them, so the features learn more about the neighbors but eventually their own features fade away and lose information.\n",
    "2)  If we use  a larger hidden dimension, the input dimension would aggregate into a higher dimension than 16. So if the hidden dimension is 64, each vector would be represented with a feature size of 64 rather than 16. This gives the model more information and has a better performance. But if the hidden dimension is too high, i.e., very close to the input dimension, the model might just overfit and memorize everything rather than learning and generalizing to the test data.\n",
    "4) If we train only on 10% of the data, it won't have enough information on it, i.e., the connections between nodes that should be present would not be counted. So the accuracy would be extremely low on the test set. If the training iterations are high, it might even overfit to the training data.\n",
    "5) RMSprop had a lower convergence speed, and the accuracy was lower as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e659b1a8-c737-4d7f-b28d-5ad999f81f3a",
   "metadata": {},
   "source": [
    "## EC ##\n",
    "1) Since dropout drops random elements during training, it regularizes the model by adding randomness to it. So it reduces overfitting and increases generalization. However, if the dropout value is too high, the model might even underfit the data.\n",
    "2) Mean pooling would eliminate one of the most important features of each node, i.e., the number of neighbors it has. This would give treat two nodes with the same average over their messages, even though one has significantly more neighbors than the other.\n",
    "3) Feature extraction using Node2Vec before passing data into GCN should improve the overall performance as Node2Vec captures the information from all across the data, and GCN would capture information in the local neighborhood of each node. This is similar to the research paper we read for course prep over summer, where they would use an LLM for feature extraction before passing these features into a GCNSA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea28be-56f9-49ed-b417-f7e5f3e57cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f016489f",
   "metadata": {},
   "source": [
    "\n",
    "### Objective: \n",
    "\n",
    "In this assignment, implement the Node2Vec algorithm, a random-walk-based GNN, to learn node embeddings. Train a classifier using the learned embeddings to predict node labels.\n",
    "\n",
    "### Dataset: \n",
    "\n",
    "Cora dataset: The dataset consists of 2,708 nodes (scientific publications) with 5,429 edges (citations between publications). Each node has a feature vector of size 1,433, and there are 7 classes (research topics).\n",
    "Skeleton Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "594df6e2-2a03-4236-a61d-530d957bc68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e053e3c6-b3ed-4e3d-bafb-d6cb6e591ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy==1.24.4 scipy==1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7371219-9809-47cd-ad5e-110e3e7e1ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: node2vec in c:\\users\\siddu\\anaconda3\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in c:\\users\\siddu\\anaconda3\\lib\\site-packages (from node2vec) (4.3.3)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in c:\\users\\siddu\\anaconda3\\lib\\site-packages (from node2vec) (1.4.2)\n",
      "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in c:\\users\\siddu\\anaconda3\\lib\\site-packages (from node2vec) (3.1)\n",
      "Collecting numpy<2.0.0,>=1.24.0 (from node2vec)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\siddu\\anaconda3\\lib\\site-packages (from node2vec) (4.66.5)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.0->node2vec)\n",
      "  Using cached scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\siddu\\anaconda3\\lib\\site-packages (from gensim<5.0.0,>=4.3.0->node2vec) (5.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\siddu\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->node2vec) (0.4.6)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "Installing collected packages: numpy, scipy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed numpy-1.26.4 scipy-1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\siddu\\AppData\\Local\\Temp\\pip-uninstall-iqnkh63a'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\siddu\\anaconda3\\Lib\\site-packages\\~.mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\siddu\\anaconda3\\Lib\\site-packages\\~~ipy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\siddu\\anaconda3\\Lib\\site-packages\\~.ipy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyfume 0.3.4 requires numpy==1.24.4, but you have numpy 1.26.4 which is incompatible.\n",
      "pyfume 0.3.4 requires scipy==1.10.1, but you have scipy 1.13.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7238416-13c6-4664-bdbd-d51b11a95a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import node2vec\n",
    "#print(node2vec.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c492a4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de4b2103c234b27849ee16304231df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_6848\\2527165497.py:57: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
      "C:\\Users\\siddu\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9437246322631836\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not Data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m     acc_lst\u001b[38;5;241m.\u001b[39mappend(test(classifier)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 43\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(dataset\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     44\u001b[0m     pred \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     45\u001b[0m     flag_is_correct \u001b[38;5;241m=\u001b[39m pred[dataset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtest_mask] \u001b[38;5;241m==\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39my[dataset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtest_mask]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not Data"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(dataset.data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        flag_is_correct = pred[dataset.data.test_mask] == dataset.data.y[dataset.data.test_mask]\n",
    "        acc = int(flag_is_correct.sum()) / int(dataset.data.test_mask.sum())\n",
    "        return acc, pred[dataset.data.test_mask], dataset.data.y[dataset.data.test_mask]\n",
    "# Training loop\n",
    "\n",
    "acc_lst = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "    acc_lst.append(test(classifier)[0])\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8fc4914-01ac-47fd-b5c0-c3cdfdfe1699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02877ddc6d294909927a2f58eb3c90b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.0141940116882324\n",
      "Epoch 10, Loss: 1.2898032665252686\n",
      "Epoch 20, Loss: 0.9287654161453247\n",
      "Epoch 30, Loss: 0.7653117775917053\n",
      "Epoch 40, Loss: 0.6859518885612488\n",
      "Epoch 50, Loss: 0.6399396657943726\n",
      "Epoch 60, Loss: 0.6095027327537537\n",
      "Epoch 70, Loss: 0.5872741937637329\n",
      "Epoch 80, Loss: 0.5700252652168274\n",
      "Epoch 90, Loss: 0.5560986995697021\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "from node2vec import Node2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Convert embeddings to a single tensor\n",
    "embedding_tensor = torch.tensor(np.array([embeddings[str(i)] for i in range(data.num_nodes)]), dtype=torch.float)\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Modified test function to use embeddings\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(embedding_tensor)\n",
    "        pred = out.argmax(dim=1)\n",
    "        flag_is_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "        acc = int(flag_is_correct.sum()) / int(data.test_mask.sum())\n",
    "        return acc\n",
    "\n",
    "# Training loop\n",
    "acc_lst = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(embedding_tensor)\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "    \n",
    "    acc_lst.append(test(classifier))\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43b8c4c7-d079-4e33-bcc2-f8900771f83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.229,\n",
       " 0.366,\n",
       " 0.419,\n",
       " 0.467,\n",
       " 0.494,\n",
       " 0.519,\n",
       " 0.54,\n",
       " 0.569,\n",
       " 0.593,\n",
       " 0.622,\n",
       " 0.636,\n",
       " 0.65,\n",
       " 0.669,\n",
       " 0.683,\n",
       " 0.698,\n",
       " 0.71,\n",
       " 0.723,\n",
       " 0.733,\n",
       " 0.745,\n",
       " 0.753,\n",
       " 0.758,\n",
       " 0.762,\n",
       " 0.765,\n",
       " 0.768,\n",
       " 0.771,\n",
       " 0.773,\n",
       " 0.779,\n",
       " 0.781,\n",
       " 0.783,\n",
       " 0.79,\n",
       " 0.791,\n",
       " 0.792,\n",
       " 0.792,\n",
       " 0.791,\n",
       " 0.792,\n",
       " 0.795,\n",
       " 0.797,\n",
       " 0.799,\n",
       " 0.799,\n",
       " 0.796,\n",
       " 0.797,\n",
       " 0.798,\n",
       " 0.802,\n",
       " 0.806,\n",
       " 0.806,\n",
       " 0.805,\n",
       " 0.805,\n",
       " 0.805,\n",
       " 0.806,\n",
       " 0.807,\n",
       " 0.806,\n",
       " 0.807,\n",
       " 0.808,\n",
       " 0.808,\n",
       " 0.809,\n",
       " 0.81,\n",
       " 0.81,\n",
       " 0.812,\n",
       " 0.813,\n",
       " 0.813,\n",
       " 0.815,\n",
       " 0.815,\n",
       " 0.816,\n",
       " 0.818,\n",
       " 0.817,\n",
       " 0.819,\n",
       " 0.82,\n",
       " 0.821,\n",
       " 0.821,\n",
       " 0.821,\n",
       " 0.821,\n",
       " 0.821,\n",
       " 0.821,\n",
       " 0.821,\n",
       " 0.821,\n",
       " 0.822,\n",
       " 0.822,\n",
       " 0.825,\n",
       " 0.826,\n",
       " 0.826,\n",
       " 0.826,\n",
       " 0.825,\n",
       " 0.825,\n",
       " 0.827,\n",
       " 0.826,\n",
       " 0.827,\n",
       " 0.827,\n",
       " 0.827,\n",
       " 0.827,\n",
       " 0.828,\n",
       " 0.828,\n",
       " 0.828,\n",
       " 0.828,\n",
       " 0.828,\n",
       " 0.828,\n",
       " 0.829,\n",
       " 0.828,\n",
       " 0.829,\n",
       " 0.829,\n",
       " 0.829]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ee022",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "Node2Vec generates node embeddings by simulating random walks on the graph. These walks capture structural properties of nodes.\n",
    "The generated embeddings are then used to train a classifier for predicting node labels.\n",
    "The Cora dataset is a benchmark graph where nodes are papers and edges are citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b3004",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "1. What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?\n",
    "2. What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?\n",
    "4. What would happen if we used directed edges instead of undirected edges for the random walks?\n",
    "5. What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?\n",
    "6. What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?\n",
    "8. What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?\n",
    "\n",
    "\n",
    "\n",
    "### Extra credit: \n",
    "1. What would happen if we increased the window size (window) for the skip-gram model? How would it affect the embedding quality?\n",
    "\n",
    "## No points, just for you to think about\n",
    "7. What would happen if we removed self-loops from the graph before training Node2Vec?\n",
    "\n",
    "9. What would happen if we applied normalization to the node embeddings before feeding them to the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa6c90-ee2b-4e0e-b96a-86aecfbe7b9c",
   "metadata": {},
   "source": [
    "## Answers ##\n",
    "1) If we increase the number of walks, we would get more embeddings from node2vec, so the model has more data to train from. This should theoretically increase the accuracy. However, if num_walks is too high, the model will memorize the entire graph and overfit the data.\n",
    "2) Reducing the walk length would give us the same number of embeddings, but the sizes of these embeddings would be smaller, i.e, we would have less features. This means the model would learn more local  information and less global. This could work well in tasks like recommender systems, and higher walk lenghts would work for overall structure detections tasks such as graph classification.\n",
    "3) Directed edges would have better accuracy in tasks which care about directionality, such as emails, where a person A sending an email to B doesn't mean B is sending the same email to A. Undirected egdes would be better for tasks which don't care about directionality, such as social networks where A being B's friend is the same as B being A's friend.\n",
    "4) If we use more features, the accuracy should be higher, unless all of them contain unnecessary information and don't help the model learn any more than it already did.\n",
    "5) I think the classifier performance would decrease with an increase in the number of classes. To get a better accuracy, you would have to increase the number of training iterations for the model to learn about the entire data.\n",
    "6) This case is similar to increasing the walk length. Increasing the size of embeddings means that the model has more to learn about each node. If the model has a high node dievrsity, this would improve the accuracy, but since we're passing in more data to the model, the training would take longer time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c548f62-368f-4edc-8814-9f4146b8e7c0",
   "metadata": {},
   "source": [
    "## EC ##\n",
    "1) I believe that the result would be same as that of increasing walk length. Since the window size decides how many neighbors encountered in each walk we look at, a higher window size means the model would be learning more global information, and lower window size would help it learn more local information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebee55a-2218-4538-986d-e1edd349aaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdbfa7-aa58-47c9-856f-fce7d9b23976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
